
@article{jarvelainen_perception-based_nodate,
	title = {Perception-based control of vibrato parameters in string instrument synthesis},
	language = {en},
	author = {Jarvelainen, Hanna},
	file = {Jarvelainen - Perception-based control of vibrato parameters in .pdf:/Users/vincemad/Zotero/storage/TVSKAWHF/Jarvelainen - Perception-based control of vibrato parameters in .pdf:application/pdf},
}


@article{chen_electric_2015,
	title = {{ELECTRIC} {GUITAR} {PLAYING} {TECHNIQUE} {DETECTION} {IN} {REAL}-{WORLD} {RECORDINGS} {BASED} {ON} {F0} {SEQUENCE} {PATTERN} {RECOGNITION}},
	abstract = {For a complete transcription of a guitar performance, the detection of playing techniques such as bend and vibrato is important, because playing techniques suggest how the melody is interpreted through the manipulation of the guitar strings. While existing work mostly focused on playing technique detection for individual single notes, this paper attempts to expand this endeavor to recordings of guitar solo tracks. Speciﬁcally, we treat the task as a time sequence pattern recognition problem, and develop a twostage framework for detecting ﬁve fundamental playing techniques used by the electric guitar. Given an audio track, the ﬁrst stage identiﬁes prominent candidates by analyzing the extracted melody contour, and the second stage applies a pre-trained classiﬁer to the candidates for playing technique detection using a set of timbre and pitch features. The effectiveness of the proposed framework is validated on a new dataset comprising of 42 electric guitar solo tracks without accompaniment, each of which covers 10 to 25 notes. The best average F-score achieves 74\% in two-fold cross validation. Furthermore, we also evaluate the performance of the proposed framework for bend detection in ﬁve studio mixtures, to discuss how it can be applied in transcribing real-world electric guitar solos with accompaniment.},
	language = {en},
	author = {Chen, Yuan-Ping and Su, Li and Yang, Yi-Hsuan},
	year = {2015},
	file = {Chen et al. - 2015 - ELECTRIC GUITAR PLAYING TECHNIQUE DETECTION IN REA.pdf:/Users/vincemad/Zotero/storage/UQ7W7M7G/Chen et al. - 2015 - ELECTRIC GUITAR PLAYING TECHNIQUE DETECTION IN REA.pdf:application/pdf},
}


@inproceedings{nishikawa_proficiency_2021,
	address = {Oita, Japan},
	title = {Proficiency {Estimation} {Method} in {Vibrato}: {A} {Special} {Technique} of an {Electric} {Guitar}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72816-714-5},
	shorttitle = {Proficiency {Estimation} {Method} in {Vibrato}},
	url = {https://ieeexplore.ieee.org/document/9651606/},
	doi = {10.1109/ICIIBMS52876.2021.9651606},
	abstract = {Many systems that provide automatic evaluation and feedback of electric guitar have been developed. However, they have a serious weakness, only “timing” and “pitch” are the focus of users performance in evaluation. In fact, a wide range of factors are involved in the evaluation. In order to solve this problem, previous studies used acoustic features for automatic evaluation of single-note playing. On the other hand, these are not possible to evaluate the sound using the special technique of guitar. In this study, we proposed a method for automatically estimating the proficiency of vibrato. As the method, we extracted the acoustic features focusing on the peak of Mel fundamental frequency and regressed the evaluation values using a support vector machine; SVM. As the result, we were able to perform regression with a coefficient of determination 0.68. This result indicates extracted features are highly relevant to evaluation values by person.},
	language = {en},
	urldate = {2024-04-09},
	booktitle = {2021 6th {International} {Conference} on {Intelligent} {Informatics} and {Biomedical} {Sciences} ({ICIIBMS})},
	publisher = {IEEE},
	author = {Nishikawa, Yuta and Matsumura, Toshie},
	month = nov,
	year = {2021},
	pages = {20--21},
	file = {Nishikawa and Matsumura - 2021 - Proficiency Estimation Method in Vibrato A Specia.pdf:/Users/vincemad/Zotero/storage/35LG9PPD/Nishikawa and Matsumura - 2021 - Proficiency Estimation Method in Vibrato A Specia.pdf:application/pdf},
}

@article{article,
author = {Seye, Elina and Mashino, Ako},
year = {2021},
month = {01},
pages = {25-45},
title = {The Corporeality of Sound and Movement in Performance},
volume = {9},
journal = {World of Music}
}

@inbook{inbook,
author = {Tanaka, Shogo},
year = {2011},
month = {01},
pages = {149-157},
title = {The notion of embodied knowledge},
isbn = {978-1553222408}
}
@article{Schyff,
author = {van der Schyff, Dylan and Schiavio, Andrea and Walton, Ashley and Velardo, Valerio and Chemero, Anthony},
year = {2018},
month = {09},
pages = {1-18},
title = {Musical creativity and the embodied mind: Exploring the possibilities of 4E cognition and dynamical systems theory},
volume = {1},
doi = {10.1177/2059204318792319}
}

@incollection{jensenius_gestures_2021,
	title = {Gestures in ensemble performance},
	isbn = {978-0-19-886076-1 978-0-19-189299-8},
	url = {https://academic.oup.com/book/41169/chapter/350540145},
	abstract = {Gestures, defined as meaning-bearing bodily actions, play important and varied roles in ensemble performance. This chapter discusses how the term “gesture” differs from physical “motion” and perceived “action.” The functional differences between sound-producing, sound-facilitating, sound-accompanying, and communicative actions are presented, alongside how these can be performed and/or perceived as meaningbearing gestures. The role of gestures in ensemble performance is examined from four perspectives: (1) ensemble size and setup; (2) the musical degrees of freedom of the ensemble; (3) the musical leadership; and (4) the role of machines in the musicianship. It is argued that the use of gestures varies between different types of ensembles and musical genres. The common denominator is the need for meaning-bearing bodily communication between performers, with such gestures also playing an important part in the musical communication with the audience.},
	language = {en},
	urldate = {2024-02-21},
	booktitle = {Together in {Music}},
	publisher = {Oxford University Press},
	author = {Jensenius, Alexander Refsum and Erdem, Cagrı},
	collaborator = {Jensenius, Alexander Refsum and Erdem, Cagrı},
	month = nov,
	year = {2021},
	doi = {10.1093/oso/9780198860761.003.0014},
	pages = {109--118},
	file = {Jensenius and Erdem - 2021 - Gestures in ensemble performance.pdf:/Users/vincemad/Zotero/storage/XFD4Z4LQ/Jensenius and Erdem - 2021 - Gestures in ensemble performance.pdf:application/pdf},
}


@incollection{miranda_interactive_2021,
	address = {Cham},
	title = {Interactive {Machine} {Learning} of {Musical} {Gesture}},
	isbn = {978-3-030-72115-2 978-3-030-72116-9},
	url = {https://link.springer.com/10.1007/978-3-030-72116-9_27},
	language = {en},
	urldate = {2024-02-16},
	booktitle = {Handbook of {Artificial} {Intelligence} for {Music}},
	publisher = {Springer International Publishing},
	author = {Visi, Federico Ghelli and Tanaka, Atau},
	editor = {Miranda, Eduardo Reck},
	year = {2021},
	doi = {10.1007/978-3-030-72116-9_27},
	pages = {771--798},
	file = {Visi and Tanaka - 2021 - Interactive Machine Learning of Musical Gesture.pdf:/Users/vincemad/Zotero/storage/DYY5M7YV/Visi and Tanaka - 2021 - Interactive Machine Learning of Musical Gesture.pdf:application/pdf;Visi and Tanaka - 2021 - Interactive Machine Learning of Musical Gesture.pdf:/Users/vincemad/Zotero/storage/X6MA94LL/Visi and Tanaka - 2021 - Interactive Machine Learning of Musical Gesture.pdf:application/pdf;Visi and Tanaka - 2021 - Interactive Machine Learning of Musical Gesture.pdf:/Users/vincemad/Zotero/storage/B5A6RRWB/Visi and Tanaka - 2021 - Interactive Machine Learning of Musical Gesture.pdf:application/pdf},
}

@article{strauss_extensible_2023,
	title = {Extensible {Embodied} {Knowledge}: {Bridging} {Performance} {Practice} and {Intelligent} {Performance} {System} {Design}},
	abstract = {A wealth of embodied knowledge has been articulated and documented in music performance practice literature. However, this documentation is usually aimed at instrument performers, and not packaged in such a way that it can be extensible to designers of computational Interactive Performance Systems (IPS). We extract meaningful dimensions from literature relating to instrument-specific kinematics, and describe the sonic and felt dimensions of specific technique. To aid a bottom-up design process, we provide a graphical representation of this articulated embodied knowledge in the form of a novel dimension space. In addition, we discuss possible solutions for how we can access and extend this embodied knowledge computationally. The resulting dimension space seeks to provide a clear understanding of what exactly we might extend through design. This research lays a foundation for future design work of IPS such as co-creative systems, interactive music systems, musical agents, and augmented instruments. In particular, this paper documents the ideation phase of our emerging IPS design for a professional violist. As such, this paper serves as an example of how we can extract and organise embodied knowledge from music performance, towards inscribing computational designs with embodied knowledge of music performance.},
	language = {en},
	author = {Strauss, Lucy and Yee-King, Matthew},
	year = {2023},
	file = {Strauss and Yee-King - 2023 - Extensible Embodied Knowledge Bridging Performanc.pdf:/Users/vincemad/Zotero/storage/PX5AEPCY/Strauss and Yee-King - 2023 - Extensible Embodied Knowledge Bridging Performanc.pdf:application/pdf},
}

@article{van_nort_mapping_2014,
	title = {Mapping {Control} {Structures} for {Sound} {Synthesis}: {Functional} and {Topological} {Perspectives}},
	volume = {38},
	issn = {0148-9267, 1531-5169},
	shorttitle = {Mapping {Control} {Structures} for {Sound} {Synthesis}},
	url = {https://direct.mit.edu/comj/article/38/3/6-22/94472},
	doi = {10.1162/COMJ_a_00253},
	abstract = {This article contributes a holistic conceptual framework for the notion of "mapping" that extends the classical view of mapping as parameter association. In presenting this holistic approach to mapping techniques, we apply the framework to existing works from the literature as well as to new implementations that consider this approach in their construction. As any mapping control structure for a given digital instrument is determined by the musical context in which it is used, we present musical examples that relate the relatively abstract realm of mapping design to the physically and perceptually grounded notions of control and sonic gesture. Making this connection allows mapping to be more clearly seen as a linkage between a physical action and a sonic result. In this sense, the purpose of this work is to translate the discussion on mapping so that it links an abstract and formalized approach—intended for representation and conceptualization—with a viewpoint that considers mapping in its role as a perceived correspondence between physical materials (i.e., those that act on controllers and transducers) and sonic events. This correspondence is, at its heart, driven by our cognitive and embodied understanding of the acoustic world.},
	language = {en},
	number = {3},
	urldate = {2024-02-21},
	journal = {Computer Music Journal},
	author = {Van Nort, Doug and Wanderley, Marcelo M. and Depalle, Philippe},
	month = sep,
	year = {2014},
	pages = {6--22},
	file = {Van Nort et al. - 2014 - Mapping Control Structures for Sound Synthesis Fu.pdf:/Users/vincemad/Zotero/storage/MECLB6PS/Van Nort et al. - 2014 - Mapping Control Structures for Sound Synthesis Fu.pdf:application/pdf},
}
